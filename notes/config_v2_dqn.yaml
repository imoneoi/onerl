env:
  import: onerl.examples.atari_env
  name: AtariDeepmind
  params:
    game: Breakout
  frame_stack: 4

  policy_batch_size: 32

algorithm:
  import: onerl.algorithm
  name: DQN
  network:
    feature_extractor:
      import: onerl.network
      name: PreactResnet
  params:
    replay_buffer_size: 1000000

    lr: 0.001
    batch_size: 32
    gamma: 0.99

    target_update_freq: 500

    eps_start: 1.0
    eps_final: 0.05
    eps_final_steps: 1000000

nodes@train:
  # env stepping
  Env:
    num: 8
  Policy:
    num: 1
    devices:
      - cuda:0
  Scheduler:
    num: 1  # only 1

nodes@test:
  Env:
    num: 1
    param:
      clip_rewards: False
  Policy:
    num: 1
    devices: [cuda:0]
  Scheduler:
    num: 1

nodes@global:
  # training
  ReplayBuffer:
    num: 1  # only 1
  Sampler:
    num: 1  # num sampler = num optimizer
  Optimizer:
    num: 1
    update_interval: 5.0  # seconds
    devices: [cuda:0]

  # system nodes
  # do not need to modify
  Logger:
    num: 1
  Validator:
    num: 1
