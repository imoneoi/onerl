$global:
  # Profiling
  # profile: True
  # profile_log_path: profile_log

  # Environment
  env:
    import: examples.envs.mujoco_env
    name: create_mujoco_env
    params:
      name: HalfCheetah-v3
    frame_stack: 1

  # Algorithm
  algorithm:
    import: onerl.algorithms
    name: TD3Algorithm
    network:  # Network architecture
      feature_extractor:
        import: torch.nn
        name: Flatten
      actor:
        import: onerl.networks
        name: MLP
        params:
          norm_type: none
          input_dims: 17  # N(state)
          num_hidden: [256, 256]
          output_dims: 6  # N(action)
      critic1:
        import: onerl.networks
        name: MLP
        params:
          norm_type: none
          input_dims: 23  # N(state) + N(action)
          num_hidden: [256, 256]
          output_dims: 1
      critic2:
        import: onerl.networks
        name: MLP
        params:
          norm_type: none
          input_dims: 23  # N(state) + N(action)
          num_hidden: [256, 256]
          output_dims: 1

    params:  # Algorithm hyper-parameters
      replay_buffer_size: 1000000
      batch_size: 256

  nodes:  # Metric recording node here
    MetricNode:
      num: 1

$train:
  # Training node configuration
  env:
    params:
      sleep_time: 0.0025  # Sleep in envs to make sampling slower, keeping update-to-data ratio

  nodes:
    EnvNode:
      num: 1
    PolicyNode:
      num: 1
      batch_size: 1
      devices: [cpu]
    SchedulerNode:
      num: 1
    ReplayBufferNode:
      num: 1
    SamplerNode:
      num: 1
    OptimizerNode:
      num: 1
      update_interval: 0.1
      devices: [cuda:0]

$test:
  # Testing node configuration
  nodes:
    EnvNode:
      num: 1
    PolicyNode:
      num: 1
      batch_size: 1
      devices: [cpu]
      do_tick: False  # Required, prevent test steps to be counted in total steps
      optimizer_namespace: $train  # Required, specify where to load parameters
    SchedulerNode:
      num: 1
